{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from num2words import num2words\n",
    "import nltk\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name ='Assignment3_IR/20_newsgroups'\n",
    "files_path=[]\n",
    "total_doc = 0\n",
    "folders = ['comp.graphics','sci.med','talk.politics.misc','rec.sport.hockey','sci.space']\n",
    "for folder in folders:\n",
    "    for files in (os.listdir(folder_name+'/'+folder)):\n",
    "        i=str(folder_name+'/'+folder+'/'+files)\n",
    "        total_doc+=1\n",
    "        files_path.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(files_path),total_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id=[words for words in range(1,len(files_path)+1)]\n",
    "dic = dict(zip(files_path,doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Assignment3_IR/20_newsgroups'\n",
    "mapping = {}\n",
    "itr = 1\n",
    "for i in range(len(folders)):\n",
    "    for files in os.listdir(directory+'/'+folders[i]):\n",
    "        mapping[itr] = folders[i]+\" \"+files\n",
    "        itr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHeader(text):\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    metadata_removed_text = \"\"\n",
    "    text_list = []\n",
    "    for i in range(1 , len(paragraphs)):\n",
    "        metadata_removed_text = metadata_removed_text + paragraphs[i]\n",
    "    return metadata_removed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "    token_list=[]\n",
    "    ps = nltk.PorterStemmer() \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = removeHeader(text)\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    punc = string.punctuation\n",
    "    tokens = [i for i in text if not i in (stop_words and punc)]\n",
    "    for word in tokens:\n",
    "        w=word.translate(str.maketrans('','',string.punctuation))\n",
    "        if len(w)>1 and w!= '' and w not in stop_words:\n",
    "            w=ps.stem(w)\n",
    "            if w.isnumeric():\n",
    "                w = num2words(w)\n",
    "                token_list.append(w)\n",
    "            else:\n",
    "                token_list.append(w)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(text):\n",
    "    token_list=[]\n",
    "    ps = nltk.PorterStemmer() \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #text = removeHeader(text)\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    punc = string.punctuation\n",
    "    tokens = [i for i in text if not i in (stop_words and punc)]\n",
    "    for word in tokens:\n",
    "        w=word.translate(str.maketrans('','',string.punctuation))\n",
    "        if len(w)>1 and w!= '' and w not in stop_words:\n",
    "            w=ps.stem(w)\n",
    "            if w.isnumeric():\n",
    "                w = num2words(w)\n",
    "                token_list.append(w)\n",
    "            else:\n",
    "                token_list.append(w)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadpkl():\n",
    "    picfile = open('Assignment4_Q1.pkl','rb')\n",
    "    picfile=pickle.load(picfile)\n",
    "    pos=picfile[0]\n",
    "    return pos\n",
    "text_dic = loadpkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dic = loadpkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dict={}\n",
    "for term in text_dic:\n",
    "    tf_dict[term]={}\n",
    "    for lst in text_dic[term]:\n",
    "        tf_dict[term][lst[0]]=lst[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf={}\n",
    "for term in text_dic:\n",
    "    idf[term] = math.log(total_doc/len(text_dic[term]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {}\n",
    "for term in tf_dict:\n",
    "    tf_dict_val = 0\n",
    "    tf_idf_inner = {}\n",
    "    for doc in tf_dict[term]:\n",
    "        tf_dict_val = tf_dict[term][doc] * idf[term]\n",
    "        tf_idf_inner[doc] = tf_dict_val\n",
    "    tf_idf[term] = tf_idf_inner \n",
    "print(len(tf_idf))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Enter query: \")\n",
    "k=input(\"enter k: \")\n",
    "k=int(k)\n",
    "pre_query=preprocess_query(query)\n",
    "pre_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = {}\n",
    "for term in pre_query:\n",
    "    if term not in tf:\n",
    "        tf[term] = pre_query.count(term)/len(pre_query)\n",
    "query_tf_idf={}\n",
    "for term in pre_query:\n",
    "    if term not in text_dic:\n",
    "        print(\"out of vocab\",term)\n",
    "    else:\n",
    "        if term not in query_tf_idf:\n",
    "            query_tf_idf[term]=tf[term]*idf[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(a,b):\n",
    "    a=set(a)\n",
    "    b=set(b)\n",
    "    c=a|b\n",
    "    c=list(c)\n",
    "    #print(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dic={}\n",
    "for term in text_dic:\n",
    "    doc_id = []\n",
    "    for lst in text_dic[term]:\n",
    "        doc_id.append(lst[0])\n",
    "    doc_dic[term] = doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for term in pre_query:\n",
    "    b=[]\n",
    "    for lst in text_dic[term]:\n",
    "        b.append(lst[0])\n",
    "    #print(len(b))\n",
    "    a = union(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryVector():\n",
    "    query_vector = []\n",
    "    for term in text_dic:\n",
    "        if term in pre_query:\n",
    "            query_vector.append(query_tf_idf[term])\n",
    "        else:\n",
    "            query_vector.append(0)\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadpkl():\n",
    "    picfile = open('A4_Q1.pkl','rb')\n",
    "    picfile=pickle.load(picfile)\n",
    "    pos=picfile[0]\n",
    "    return pos\n",
    "docs_dic = loadpkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "n_items = take(10,docs_dic.items())\n",
    "print(n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docs in docs_dic:\n",
    "        docs_dic[docs] = np.asarray(docs_dic[docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(Q):\n",
    "    ranked_doc={}\n",
    "    for docs in a:\n",
    "        vec1 = np.dot(docs_dic[docs],Q)\n",
    "        vec1 = np.dot(vec1,vec1)\n",
    "        temp = np.sum(vec1)\n",
    "        value_DQ = math.sqrt(temp)\n",
    "        document = np.dot(docs_dic[docs],docs_dic[docs])\n",
    "        temp1 = np.sum(document)\n",
    "        d = math.sqrt(temp1)\n",
    "        query = np.dot(Q,Q)\n",
    "        temp2 = np.sum(query)\n",
    "        q = math.sqrt(temp2)\n",
    "        try:\n",
    "            ranked_doc[docs] = value_DQ/d*q\n",
    "        except:\n",
    "            ranked_doc[docs] = 0\n",
    "    sorted_dictionary = sorted(ranked_doc.items(), reverse = True ,key = lambda x:x[1])\n",
    "    sorted_dictionary =  [i for i in sorted_dictionary] \n",
    "    return sorted_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapp_doc(ranked_doc):\n",
    "    sorted_dictionary = ranked_doc\n",
    "    sorted_dictionary= sorted_dictionary[:(k)]\n",
    "    ret_doc=[]\n",
    "    orgDoc = []\n",
    "    for i in sorted_dictionary:\n",
    "        ret_doc.append(i[0])\n",
    "    for doc in ret_doc:\n",
    "        #print(doc)\n",
    "        orgDoc.append(mapping[doc])\n",
    "    return orgDoc,ret_doc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = queryVector()\n",
    "query_vector = np.asarray(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_feedback(retrived_doc):\n",
    "    enter_rel = input(\"Enter number of relevant docs: \").split(',')\n",
    "    rel_doc = [retrived_doc[int(i)] for i in enter_rel]\n",
    "    non_rel = list(set(retrived_doc)-set(rel_doc))\n",
    "    return rel_doc,non_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_centroid(docs,const):\n",
    "    vec = np.asarray(docs_dic[docs[0]])\n",
    "    #print(vec)\n",
    "    for d_Id in range(1,len(docs)):\n",
    "        vec+=docs_dic[docs[d_Id]]\n",
    "    vec = (const/len(docs)) * vec\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_query(alpha,beta,gamma,query_vector,rel_docs,non_rel_docs): \n",
    "    query_vector = np.multiply(alpha,query_vector)\n",
    "    D_r = calc_centroid(rel_docs,beta)\n",
    "    D_nr = calc_centroid(non_rel_docs,gamma)\n",
    "    sum1 = np.add(query_vector,D_r)\n",
    "    q_m = np.subtract(sum1,D_nr)\n",
    "    return q_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MAP(relevant_doc,retrieved_doc):\n",
    "    t=1\n",
    "    r=0\n",
    "    AP=0\n",
    "    for i in range(1,len(retrieved_doc)):\n",
    "        if retrieved_doc[i] in relevant_doc:\n",
    "            r+=1\n",
    "            p=r/t\n",
    "            AP+=p\n",
    "        t+=1\n",
    "    MAP = AP/len(relevant_doc)\n",
    "    return MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_TSNE(relevant_doc,non_relevant_doc,vector_q):\n",
    "    X = []\n",
    "    labels = []\n",
    "    \n",
    "    for d_Id in relevant_doc:\n",
    "        X.append(docs_dic[d_Id])\n",
    "        labels.append(0)\n",
    "    for d_Id in non_relevant_doc:\n",
    "        X.append(docs_dic[d_Id])\n",
    "        labels.append(1)\n",
    "    X.append(query_vector)\n",
    "    labels.append(2) \n",
    "    X = np.asarray(X)\n",
    "    X_embedded = TSNE(n_components = 2, verbose=0, random_state=0).fit_transform(X)\n",
    "    X_embedded = np.asarray(X_embedded)\n",
    "    print(\"Xshape: \",X.shape)\n",
    "    x_axis = X_embedded[:,0]\n",
    "    y_axis = X_embedded[:,1]\n",
    "    #plt.scatter(x_axis,y_axis,c=labels, s=60, alpha =0.8)\n",
    "    colormap = np.array(['tab:red', 'tab:blue', 'tab:green'])\n",
    "    groups = np.array([\"R\", \"N-R\", \"Query\"])  \n",
    "    plt.scatter(x_axis, y_axis,c=colormap[labels], label = groups)\n",
    "    plt.title(\"Rocchio\")\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_retrieved(ret_doc,rel_doc,orgDoc):\n",
    "    print(\"Top docs: \")\n",
    "    for i in range(len(ret_doc)):\n",
    "        if ret_doc[i] in rel_doc:\n",
    "            print(i,ret_doc[i],orgDoc[i],'*')\n",
    "        else:\n",
    "            print(i,ret_doc[i],orgDoc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map=[]\n",
    "rank = cosine_sim(query_vector)\n",
    "orgDoc,ret_doc=mapp_doc(rank)\n",
    "for i in range(len(ret_doc)):\n",
    "    print(i,ret_doc[i],orgDoc[i])\n",
    "rel_doc,non_rel = user_feedback(ret_doc)\n",
    "precision = []\n",
    "recall = []\n",
    "total_relevant = 1000\n",
    "total_retrieved = 0\n",
    "relevant_retrieved = 0 \n",
    "for i in range(len(ret_doc)):\n",
    "    if(ret_doc[i] in rel_doc):\n",
    "        relevant_retrieved+=1\n",
    "    total_retrieved+=1\n",
    "    p = relevant_retrieved/total_retrieved\n",
    "    r = relevant_retrieved/total_relevant\n",
    "    precision.append(p)\n",
    "    recall.append(r)\n",
    "MAP = compute_MAP(rel_doc,ret_doc)\n",
    "print('MAP: ',MAP)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.plot(recall, precision)\n",
    "plt.show()\n",
    "plot_TSNE(rel_doc,non_rel,query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 0\n",
    "\n",
    "for iteration in range(1,5):\n",
    "    print(\"iteration\",iteration)\n",
    "    query_vector = modified_query(1,0.75,0.25,query_vector,rel_doc,non_rel)\n",
    "    for i in range(len(query_vector)):\n",
    "        if(query_vector[i]<0):\n",
    "            query_vector[i]=0\n",
    "    retrived_docs = cosine_sim(query_vector)\n",
    "    orgDoc,ret_doc=mapp_doc(retrived_docs) \n",
    "    top_retrieved(ret_doc,rel_doc,orgDoc)\n",
    "    rel_doc,non_rel = user_feedback(ret_doc)\n",
    "    print(rel_doc,non_rel)\n",
    "    precision_m = []\n",
    "    recall_m = []\n",
    "    total_relevant_m = 1000\n",
    "    total_retrieved_m = 0\n",
    "    relevant_retrieved_m = 0 \n",
    "    for i in range(len(ret_doc)):\n",
    "        if(ret_doc[i] in rel_doc):\n",
    "            relevant_retrieved_m+=1\n",
    "        total_retrieved_m+=1\n",
    "        p_m = relevant_retrieved_m/total_retrieved_m\n",
    "        r_m = relevant_retrieved_m/total_relevant_m\n",
    "        precision_m.append(p_m)\n",
    "        recall_m.append(r_m)\n",
    "    MAP_m = compute_MAP(rel_doc,ret_doc)\n",
    "    Map.append(MAP_m)\n",
    "    print('MAP: ',MAP_m,Map)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.plot(recall_m, precision_m)\n",
    "    plt.show()\n",
    "    plot_TSNE(rel_doc,non_rel,query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i in Map:\n",
    "    sum+=i\n",
    "sum = sum/len(Map)\n",
    "print(\"MAP\",sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
